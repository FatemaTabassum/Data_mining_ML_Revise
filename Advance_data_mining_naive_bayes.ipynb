{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8GD4/tnRVqspXsaMYLsJq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FatemaTabassum/Data_mining_ML_Revise/blob/main/Advance_data_mining_naive_bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For theoretical knowledge you can study following link\n",
        "\n",
        "https://www.geeksforgeeks.org/machine-learning/naive-bayes-classifiers/"
      ],
      "metadata": {
        "id": "ZmJzRRDY8kqC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hsZS8h-XM9w"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "import sys\n",
        "import collections\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "import sys\n",
        "import collections\n",
        "import math\n",
        "\n",
        "class NaiveBayesClassifier:\n",
        "  def __init__(self) -> None:\n",
        "    self.prior_prob_dict = {}\n",
        "    self.keep_count_dict = {}\n",
        "    self.class_1_cnt = 0\n",
        "    self.class_2_cnt = 0\n",
        "    self.class_1_identifier = 0\n",
        "    self.class_2_identifier = 1\n",
        "    self.dataset_train = []\n",
        "    self.labels_train = []\n",
        "    self.dataset_test = []\n",
        "    self.labels_test = []\n",
        "    self.max_attribute = 0\n",
        "    self.attribute_value_counts = defaultdict(set) # To store unique values for each attribute\n",
        "\n",
        "  #r red from files\n",
        "  def read_files(self, train_filename, test_filename):\n",
        "    with open(train_filename, 'r') as f:\n",
        "      train_set = f.readlines()\n",
        "    with open(test_filename, 'r') as f:\n",
        "      test_set = f.readlines()\n",
        "    return train_set, test_set\n",
        "\n",
        "  # create a list of dataset\n",
        "  def create_dataset(self, raw_data_set):\n",
        "    labels = []\n",
        "    dataset = []\n",
        "    max_id = 0\n",
        "    for line in raw_data_set:\n",
        "      res_val = re.split(r'[ ]', line.strip()) # Use strip() to remove leading/trailing whitespace\n",
        "      ind = 0\n",
        "      attribute_dict = {} # Use a dictionary to store attributes\n",
        "      for val in res_val:\n",
        "        if ind == 0:\n",
        "          if val == '+1':\n",
        "            labels.append(self.class_1_identifier)      # +1 is categorized as class 0\n",
        "          else:\n",
        "            labels.append(self.class_2_identifier)      # -1 is categorized as class 1\n",
        "        else:\n",
        "          if ':' in val: # Ensure the value contains ':' before splitting\n",
        "            v = re.split(r'\\:', val)\n",
        "            id = int(v[0])          # index starts from 1\n",
        "            num = int(v[1])\n",
        "            max_id = max(max_id, id)\n",
        "            attribute_dict[id] = num # Store attribute in dictionary\n",
        "            self.attribute_value_counts[id].add(num) # Track unique values for smoothing\n",
        "        # incrementing index for attribute\n",
        "        ind +=1\n",
        "      dataset.append(attribute_dict)\n",
        "    self.max_attribute = max_id # Update the max_attribute instance variable\n",
        "    return labels, dataset\n",
        "\n",
        "\n",
        "  def compute_prior_prob(self, labels):\n",
        "    self.class_1_cnt = labels.count(self.class_1_identifier)\n",
        "    self.class_2_cnt = labels.count(self.class_2_identifier)\n",
        "    total_instances = len(labels)\n",
        "    if total_instances > 0:\n",
        "        self.prior_prob_dict[self.class_1_identifier] = self.class_1_cnt / total_instances\n",
        "        self.prior_prob_dict[self.class_2_identifier] = self.class_2_cnt / total_instances\n",
        "    else:\n",
        "        # Handle case with no data to avoid division by zero\n",
        "        self.prior_prob_dict[self.class_1_identifier] = 0\n",
        "        self.prior_prob_dict[self.class_2_identifier] = 0\n",
        "    return self.prior_prob_dict\n",
        "\n",
        "\n",
        "\n",
        "  def classifier_train(self, dataset, labels):\n",
        "    # Initialize keep_count_dict as a defaultdict to handle missing keys easily\n",
        "    self.keep_count_dict = defaultdict(lambda: defaultdict(int)) # Nested defaultdict for class -> attribute_id -> value -> count\n",
        "\n",
        "    for i in range(0, len(dataset)):\n",
        "        class_label = labels[i]\n",
        "        attributes = dataset[i]\n",
        "        for attr_id, value in attributes.items():\n",
        "            self.keep_count_dict[class_label][attr_id][value] += 1\n",
        "\n",
        "    #print(self.keep_count_dict) # Optional: for debugging\n",
        "\n",
        "\n",
        "\n",
        "  def _get_log_conditional_probability(self, class_label, attribute_id, attribute_value):\n",
        "    \"\"\"Calculates the log conditional probability of an attribute value given a class.\"\"\"\n",
        "    class_count = self.class_1_cnt if class_label == self.class_1_identifier else self.class_2_cnt\n",
        "\n",
        "    # Get the count of the attribute value for the given class and attribute ID\n",
        "    count_attribute_value = self.keep_count_dict.get(class_label, {}).get(attribute_id, {}).get(attribute_value, 0)\n",
        "\n",
        "    # Determine the number of possible values for this attribute based on training data\n",
        "    num_possible_values = len(self.attribute_value_counts[attribute_id]) if attribute_id in self.attribute_value_counts else 2 # Assume binary if not seen\n",
        "\n",
        "    # Apply Laplace smoothing (Add-1 smoothing)\n",
        "    smoothed_count = count_attribute_value + 1\n",
        "    smoothing_denominator = class_count + num_possible_values\n",
        "\n",
        "    if smoothing_denominator > 0:\n",
        "        probability = smoothed_count / smoothing_denominator\n",
        "        if probability > 0:\n",
        "            return math.log(probability)\n",
        "    return 0 # Return log(a very small number) or handle appropriately if probability is zero\n",
        "\n",
        "\n",
        "\n",
        "  def compute_likelyhood(self, dataset_to_predict):\n",
        "      predicted_labels = []\n",
        "      for instance_attributes in dataset_to_predict:\n",
        "        log_likelihood_class1 = 0\n",
        "        log_likelihood_class2 = 0\n",
        "\n",
        "        # Iterate through all unique attribute IDs seen during training or in the current instance\n",
        "        all_attribute_ids = set(self.attribute_value_counts.keys()) | set(instance_attributes.keys())\n",
        "\n",
        "\n",
        "        for attribute_id in all_attribute_ids:\n",
        "          attribute_value = instance_attributes.get(attribute_id, 0) # Get the value if present, default to 0 if not\n",
        "\n",
        "          # Add log conditional probability for class 1\n",
        "          log_likelihood_class1 += self._get_log_conditional_probability(self.class_1_identifier, attribute_id, attribute_value)\n",
        "\n",
        "          # Add log conditional probability for class 2\n",
        "          log_likelihood_class2 += self._get_log_conditional_probability(self.class_2_identifier, attribute_id, attribute_value)\n",
        "\n",
        "        # Adding log of prior probabilities\n",
        "        if self.class_1_identifier in self.prior_prob_dict and self.prior_prob_dict[self.class_1_identifier] > 0:\n",
        "             log_likelihood_class1 += math.log(self.prior_prob_dict[self.class_1_identifier])\n",
        "\n",
        "        if self.class_2_identifier in self.prior_prob_dict and self.prior_prob_dict[self.class_2_identifier] > 0:\n",
        "            log_likelihood_class2 += math.log(self.prior_prob_dict[self.class_2_identifier])\n",
        "\n",
        "\n",
        "        if log_likelihood_class1 >= log_likelihood_class2:\n",
        "          predicted_labels.append(self.class_1_identifier)\n",
        "        else:\n",
        "          predicted_labels.append(self.class_2_identifier)\n",
        "\n",
        "      return predicted_labels\n",
        "\n",
        "\n",
        "  # print helper functions\n",
        "  def print_lines(self, lines):\n",
        "    for line in lines:\n",
        "      print(line)\n",
        "\n",
        "  def compute_confusion_matrix(self, original_label, computed_labels):\n",
        "      a = 0\n",
        "      d = 0\n",
        "      b = 0\n",
        "      c = 0\n",
        "      for i in range(0, len(original_label)):\n",
        "        if original_label[i] == computed_labels[i] and original_label[i] == 0:\n",
        "          a += 1\n",
        "        if original_label[i] == computed_labels[i] and original_label[i] == 1:\n",
        "          d += 1\n",
        "        if original_label[i] == 0 and computed_labels[i] == 1:\n",
        "          b += 1\n",
        "        if original_label[i] == 1 and computed_labels[i] == 0:\n",
        "          c += 1\n",
        "      print('a= ', a, 'b= ', b, 'c= ', c, 'd= ', d)"
      ],
      "metadata": {
        "id": "aIjyALT2rejR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    naiveBayes = NaiveBayesClassifier()\n",
        "\n",
        "    train_set, test_set = naiveBayes.read_file('/home/grads/liza/spring2021/assignment_3/package/breast_cancer.train', '/home/grads/liza/spring2021/assignment_3/package/breast_cancer.test' )\n",
        "    naiveBayes.print_lines(test_set)\n",
        "    max_no_attr_train, naiveBayes.labels_train, naiveBayes.dataset_train = naiveBayes.create_dataset(train_set)\n",
        "\n",
        "    max_no_attr_test, naiveBayes.labels_test , naiveBayes.dataset_test = naiveBayes.create_dataset(test_set)\n",
        "    max_attr = max(max_no_attr_train, max_no_attr_test)\n",
        "    naiveBayes.max_attr = max_attr\n",
        "    naiveBayes.compute_prior_prob(len(naiveBayes.labels_train))\n",
        "    naiveBayes.classifier_train()\n",
        "\n",
        "    #test dataset\n",
        "    computed_labels = naiveBayes.compute_likelyhood(naiveBayes.dataset_test)\n",
        "\n",
        "    print('#################### Test ########################  ')\n",
        "    print(naiveBayes.labels_test)\n",
        "    print(computed_labels)\n",
        "    naiveBayes.compute_confusion_matrix(naiveBayes.labels_test, computed_labels)\n",
        "\n",
        "\n",
        "    #train dataset\n",
        "    computed_labels = naiveBayes.compute_likelyhood(naiveBayes.dataset_train)\n",
        "    print('#################### train ########################  ')\n",
        "    print(naiveBayes.labels_train)\n",
        "    print(computed_labels)\n",
        "    naiveBayes.compute_confusion_matrix(naiveBayes.labels_train, computed_labels)\n",
        "\n",
        "\n",
        "    #print(ag.showValues())"
      ],
      "metadata": {
        "id": "XhD_YwRb4eTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Theory"
      ],
      "metadata": {
        "id": "k32nuN1y4xAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the compute_likelyhood method, the log probabilities are added in two main places:\n",
        "\n",
        "Adding the log of conditional probabilities: Inside the loop that iterates through the attributes (for attr_id in all_attribute_ids:),\n",
        "the code calculates the log of the conditional probability\n",
        "for each attribute value given the class and adds it to the class_1_log_likelihood_sum and class_2_log_likelihood_sum.\n",
        "\n",
        "\n",
        "1. **Here are the relevant lines:**\n",
        "\n",
        "          if prob_class1 > 0: # Avoid log(0)\n",
        "            class_1_log_likelihood_sum += math.log(prob_class1)\n",
        "\n",
        "and\n",
        "\n",
        "          if prob_class2 > 0: # Avoid log(0)\n",
        "            class_2_log_likelihood_sum += math.log(prob_class2)\n",
        "\n",
        "Instead of multiplying probabilities\n",
        "\n",
        "(P(F1|C) * P(F2|C) * ...), the code calculates the logarithm of each conditional\n",
        "probability\n",
        "\n",
        "(log(P(F1|C)), log(P(F2|C)), ...)\n",
        "and adds them together\n",
        "\n",
        "(log(P(F1|C)) + log(P(F2|C)) + ...).\n",
        "\n",
        "This is mathematically equivalent to multiplying the original probabilities because\n",
        "\n",
        "log(a * b) = log(a) + log(b),\n",
        "\n",
        "but it avoids potential numerical underflow when dealing with very small probability values.\n",
        "\n",
        "\n",
        "2. **Adding the log of prior probabilities:**\n",
        "\n",
        "After the loop through attributes finishes for an instance,\n",
        "the code adds the log of the prior probability of each class to the accumulated sum of log conditional probabilities.\n",
        "\n",
        "Here are the relevant lines:\n",
        "    if self.class_1_identifier in self.prior_prob_dict and self.prior_prob_dict[self.class_1_identifier] > 0:\n",
        "         class_1_log_likelihood_sum += math.log(self.prior_prob_dict[self.class_1_identifier])\n",
        "\n",
        "\n",
        "and\n",
        "\n",
        "\n",
        "    if self.class_2_identifier in self.prior_prob_dict and self.prior_prob_dict[self.class_2_identifier] > 0:\n",
        "        class_2_log_likelihood_sum += math.log(self.prior_prob_dict[self.class_2_identifier])\n",
        "\n",
        "\n",
        "This is because, according to Bayes' theorem, the posterior probability is proportional to the likelihood\n",
        "multiplied by the prior\n",
        "\n",
        "(P(Class | Features) ∝ P(Features | Class) * P(Class)).\n",
        "\n",
        "In the log domain, this becomes\n",
        "\n",
        "log(P(Class | Features)) ∝ log(P(Features | Class)) + log(P(Class)).\n",
        "\n",
        "\n",
        "The accumulated\n",
        "\n",
        "class_1_log_likelihood_sum  and  class_2_log_likelihood_sum\n",
        "\n",
        "represent\n",
        "\n",
        "log(P(Features | Class)),\n",
        "\n",
        "so the code adds\n",
        "\n",
        "log(P(Class))\n",
        "\n",
        "to them.\n",
        "\n",
        "By summing the logarithms of the conditional and prior probabilities, the code is effectively calculating the\n",
        "logarithm of the numerator of Bayes' theorem for each class. The class with the higher resulting sum in the log\n",
        "domain will also have the higher probability in the original probability domain."
      ],
      "metadata": {
        "id": "oTfYzD3jq2Cv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reason why we sum the log of the conditional probabilities instead of multiplying the original conditional probabilities"
      ],
      "metadata": {
        "id": "SPHZD7Ky8N4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason we sum the log of the conditional probabilities instead of multiplying the original conditional probabilities is to avoid numerical underflow.\n",
        "\n",
        "Here's the explanation:\n",
        "\n",
        "Naive Bayes Formula: In Naive Bayes, the core calculation for the likelihood of the features given a class is the product of the conditional probabilities of each feature given that class:\n",
        "\n",
        "\n",
        "P(Features | Class) = P(Feature1 | Class) * P(Feature2 | Class) * ... * P(FeatureN | Class)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Dealing with Many Features: In many real-world datasets, you can have a large number of features (attributes). The conditional probabilities for individual features given a class can be very small numbers (e.g., 0.0001, 0.0000001).\n",
        "\n",
        "Numerical Underflow: When you multiply many very small floating-point numbers together, the result can become so small that it underflows the precision of the computer's floating-point representation. This means the result might be rounded down to zero, even if the true probability is a very small positive number. If the likelihood for a class becomes zero due to underflow, you can't compare it meaningfully with the likelihoods of other classes, and the classifier won't be able to make a correct prediction.\n",
        "\n",
        "Using Logarithms: Logarithms have a property that helps with this:\n",
        "\n",
        "\n",
        "\n",
        "log(a * b * c) = log(a) + log(b) + log(c)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This means that the logarithm of a product is equal to the sum of the logarithms of the individual terms.\n",
        "\n",
        "Applying to Naive Bayes: By taking the logarithm of the likelihood calculation, the multiplication of many small probabilities is transformed into the summation of their logarithms:\n",
        "\n",
        "\n",
        "log(P(Features | Class)) = log(P(Feature1 | Class) * ... * P(FeatureN | Class))\n",
        "\n",
        "log(P(Features | Class)) = log(P(Feature1 | Class)) + ... + log(P(FeatureN | Class))\n",
        "\n",
        "\n",
        "\n",
        "6. Benefits:\n",
        "\n",
        "Prevents Underflow: Summing logarithms of small numbers results in a negative number (since log(x) for 0 < x < 1 is negative), but the magnitude of this negative number is much more manageable than the tiny product of the original probabilities. This avoids the underflow issue.\n",
        "Maintains Relative Order: While the actual numerical values change, the relative order of the likelihoods between different classes is preserved. If P1 > P2, then log(P1) > log(P2). So, comparing log(P(Features | Class1)) and log(P(Features | Class2)) gives the same result as comparing P(Features | Class1) and P(Features | Class2)).\n",
        "Therefore, summing the log conditional probabilities is a standard and essential technique in implementing Naive Bayes to ensure numerical stability and prevent underflow, allowing the classifier to make accurate predictions even with many features and small probabilities.\n"
      ],
      "metadata": {
        "id": "UUOkXVrR2FsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explaining this line\n",
        "\n",
        "# all_attribute_ids = set(self.attribute_value_counts.keys()) | set(attributes.keys())"
      ],
      "metadata": {
        "id": "NwRMEOkW7_j5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "all_attribute_ids = set(self.attribute_value_counts.keys()) | set(attributes.keys())\n",
        "\n",
        "Here's what it does:\n",
        "\n",
        "self.attribute_value_counts.keys():\n",
        "\n",
        "This retrieves all the unique attribute IDs that were encountered during the training process (when create_dataset was called).\n",
        "\n",
        "attributes.keys():\n",
        "\n",
        "This retrieves all the attribute IDs that are present in the current test instance being processed in the compute_likelyhood method.\n",
        "\n",
        "set(...):\n",
        "\n",
        "Both sets of keys are converted into set objects. Using sets is efficient for performing set operations.\n",
        "\n",
        "|: This is the set union operator. It combines the two sets of attribute IDs into a single set called all_attribute_ids.\n",
        "\n",
        "This line is crucial for ensuring that the classifier correctly handles attributes that might be present in the test data but were not present in a specific training instance, or vice-versa.\n",
        "\n",
        "Why is this important?\n",
        "\n",
        "In Naive Bayes, to calculate the likelihood of a data point belonging to a class, you need to consider the probabilities of all the relevant features (attributes) given that class.\n",
        "\n",
        "If an attribute was seen in the training data for a particular class, but is not present in the current test instance, we still need to account for its probability based on the training data.\n",
        "If an attribute is present in the current test instance but was not seen at all during training\n",
        "(this is less likely with a well-preprocessed dataset, but possible),\n",
        "the code still needs to iterate through it to apply smoothing.\n",
        "By creating all_attribute_ids as the union of attribute IDs from both the training data\n",
        "(specifically, those seen in attribute_value_counts)\n",
        "\n",
        "and the current test instance, the code ensures that the loop that calculates the log likelihoods\n",
        "\n",
        "(for attr_id in all_attribute_ids:)\n",
        "\n",
        "considers all relevant attributes for that instance, even if an attribute isn't explicitly listed in the sparse representation of that particular test instance. This is essential for applying smoothing correctly and avoiding zero probabilities that could arise from unseen attribute-value combinations."
      ],
      "metadata": {
        "id": "JWMQD63g1p8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rough - Only for printing"
      ],
      "metadata": {
        "id": "YpRukiuQ8a8a"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bcf73c3"
      },
      "source": [
        "# nothing related to the algo\n",
        "\n",
        "# Assuming the NaiveBayesClassifier class is defined in a previous cell\n",
        "# Let's create a sample raw dataset string\n",
        "sample_raw_data = \"\"\"+1 1:1 5:1 10:1\n",
        "-1 2:1 5:0 8:1\n",
        "+1 1:0 5:1 12:0\"\"\"\n",
        "\n",
        "# Split the sample data into lines\n",
        "sample_raw_data_set = sample_raw_data.strip().split('\\n')\n",
        "\n",
        "# Instantiate the NaiveBayesClassifier\n",
        "nb_classifier = NaiveBayesClassifier()\n",
        "\n",
        "# Call the create_dataset method with the sample data\n",
        "max_id, labels, dataset = nb_classifier.create_dataset(sample_raw_data_set)\n",
        "\n",
        "# Print the results\n",
        "print(f\"max_id: {max_id}\")\n",
        "print(f\"labels: {labels}\")\n",
        "print(\"Sample dataset entries:\")\n",
        "for i in range(min(3, len(dataset))): # Print up to 3 sample entries\n",
        "  print(f\"  {dataset[i]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11f2d41c",
        "outputId": "77d17823-b2b6-4a6a-933d-3fad77382fa7"
      },
      "source": [
        "# This creates a list with 20 zeros\n",
        "attribute_ar = [0] * 20\n",
        "\n",
        "# This prints the list\n",
        "print(attribute_ar)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X5X8CZAQ2SQ3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}