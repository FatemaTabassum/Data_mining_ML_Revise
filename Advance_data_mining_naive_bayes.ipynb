{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0820ibzllYfAlnIE2S93U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FatemaTabassum/Data_mining_ML_Revise/blob/main/Advance_data_mining_naive_bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hsZS8h-XM9w"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "import sys\n",
        "import collections\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "import sys\n",
        "import collections\n",
        "import math\n",
        "\n",
        "class NaiveBayesClassifier:\n",
        "  def __init__(self) -> None:\n",
        "    self.prior_prob_dict = {}\n",
        "    self.keep_count_dict = {}\n",
        "    self.class_1_cnt = 0\n",
        "    self.class_2_cnt = 0\n",
        "    self.class_1_identifier = 0\n",
        "    self.class_2_identifier = 1\n",
        "    self.dataset_train = []\n",
        "    self.labels_train = []\n",
        "    self.dataset_test = []\n",
        "    self.labels_test = []\n",
        "    self.max_attribute = 0\n",
        "    self.attribute_value_counts = defaultdict(set) # To store unique values for each attribute\n",
        "\n",
        "  #r red from files\n",
        "  def read_files(self, train_filename, test_filename):\n",
        "    with open(train_filename, 'r') as f:\n",
        "      train_set = f.readlines()\n",
        "    with open(test_filename, 'r') as f:\n",
        "      test_set = f.readlines()\n",
        "    return train_set, test_set\n",
        "\n",
        "  # create a list of dataset\n",
        "  def create_dataset(self, raw_data_set):\n",
        "    labels = []\n",
        "    dataset = []\n",
        "    max_id = 0\n",
        "    for line in raw_data_set:\n",
        "      res_val = re.split(r'[ ]', line.strip()) # Use strip() to remove leading/trailing whitespace\n",
        "      ind = 0\n",
        "      attribute_dict = {} # Use a dictionary to store attributes\n",
        "      for val in res_val:\n",
        "        if ind == 0:\n",
        "          if val == '+1':\n",
        "            labels.append(self.class_1_identifier)      # +1 is categorized as class 0\n",
        "          else:\n",
        "            labels.append(self.class_2_identifier)      # -1 is categorized as class 1\n",
        "        else:\n",
        "          if ':' in val: # Ensure the value contains ':' before splitting\n",
        "            v = re.split(r'\\:', val)\n",
        "            id = int(v[0])          # index starts from 1\n",
        "            num = int(v[1])\n",
        "            max_id = max(max_id, id)\n",
        "            attribute_dict[id] = num # Store attribute in dictionary\n",
        "            self.attribute_value_counts[id].add(num) # Track unique values for smoothing\n",
        "        # incrementing index for attribute\n",
        "        ind +=1\n",
        "      dataset.append(attribute_dict)\n",
        "    self.max_attribute = max_id # Update the max_attribute instance variable\n",
        "    return labels, dataset\n",
        "\n",
        "\n",
        "  def compute_prior_prob(self, labels):\n",
        "    self.class_1_cnt = labels.count(self.class_1_identifier)\n",
        "    self.class_2_cnt = labels.count(self.class_2_identifier)\n",
        "    total_instances = len(labels)\n",
        "    if total_instances > 0:\n",
        "        self.prior_prob_dict[self.class_1_identifier] = self.class_1_cnt / total_instances\n",
        "        self.prior_prob_dict[self.class_2_identifier] = self.class_2_cnt / total_instances\n",
        "    else:\n",
        "        # Handle case with no data to avoid division by zero\n",
        "        self.prior_prob_dict[self.class_1_identifier] = 0\n",
        "        self.prior_prob_dict[self.class_2_identifier] = 0\n",
        "    return self.prior_prob_dict\n",
        "\n",
        "  def classifier_train(self, dataset, labels):\n",
        "    # Initialize keep_count_dict as a defaultdict to handle missing keys easily\n",
        "    self.keep_count_dict = defaultdict(lambda: defaultdict(int)) # Nested defaultdict for class -> attribute_id -> value -> count\n",
        "\n",
        "    for i in range(0, len(dataset)):\n",
        "        class_label = labels[i]\n",
        "        attributes = dataset[i]\n",
        "        for attr_id, value in attributes.items():\n",
        "            self.keep_count_dict[class_label][attr_id][value] += 1\n",
        "\n",
        "    #print(self.keep_count_dict) # Optional: for debugging\n",
        "\n",
        "  def compute_likelyhood(self, dataset_):\n",
        "      computed_labels = []\n",
        "      for j in range(0, len(dataset_)):\n",
        "        class_1_log_likelihood_sum = 0 # Use log probabilities and sum them\n",
        "        class_2_log_likelihood_sum = 0 # Use log probabilities and sum them\n",
        "\n",
        "        attributes = dataset_[j] # Get attributes for the current instance\n",
        "\n",
        "        # Iterate through all possible attributes up to self.max_attribute\n",
        "        # This is important for smoothing, even if an attribute is not present in the current instance\n",
        "        # in the test set, we need to consider its probability based on the training data.\n",
        "        # We will iterate through all unique attribute IDs seen during training.\n",
        "        all_attribute_ids = set(self.attribute_value_counts.keys()) | set(attributes.keys())\n",
        "\n",
        "\n",
        "        for attr_id in all_attribute_ids:\n",
        "          value = attributes.get(attr_id, 0) # Get the value if present, default to 0 if not\n",
        "\n",
        "          # Determine the number of possible values for this attribute based on training data\n",
        "          num_possible_values = len(self.attribute_value_counts[attr_id]) if attr_id in self.attribute_value_counts else 2 # Assume binary if not seen\n",
        "\n",
        "          # check with class 1\n",
        "          # Use the nested defaultdict structure for lookup\n",
        "          count_class1 = self.keep_count_dict.get(self.class_1_identifier, {}).get(attr_id, {}).get(value, 0)\n",
        "\n",
        "          # Add 1 smoothing (Laplace smoothing) to avoid zero probabilities\n",
        "          smoothed_count_class1 = count_class1 + 1\n",
        "          # Calculate denominator for smoothing: total count of instances in class 1 + number of possible values for this attribute\n",
        "          smoothing_denominator_class1 = self.class_1_cnt + num_possible_values\n",
        "\n",
        "          if smoothing_denominator_class1 > 0:\n",
        "              prob_class1 = smoothed_count_class1 / smoothing_denominator_class1\n",
        "              if prob_class1 > 0: # Avoid log(0)\n",
        "                class_1_log_likelihood_sum += math.log(prob_class1)\n",
        "              else:\n",
        "                  # Handle the case where probability is still zero after smoothing (shouldn't happen with +1 smoothing and num_possible_values >= 1)\n",
        "                  # but as a safeguard, add a very small value or handle appropriately.\n",
        "                  # For simplicity, we'll just skip adding to the sum if prob is 0 after smoothing.\n",
        "                  pass\n",
        "\n",
        "\n",
        "          # check with class 2\n",
        "          count_class2 = self.keep_count_dict.get(self.class_2_identifier, {}).get(attr_id, {}).get(value, 0)\n",
        "\n",
        "          smoothed_count_class2 = count_class2 + 1\n",
        "          smoothing_denominator_class2 = self.class_2_cnt + num_possible_values\n",
        "\n",
        "          if smoothing_denominator_class2 > 0:\n",
        "              prob_class2 = smoothed_count_class2 / smoothing_denominator_class2\n",
        "              if prob_class2 > 0: # Avoid log(0)\n",
        "                class_2_log_likelihood_sum += math.log(prob_class2)\n",
        "              else:\n",
        "                  # Handle the case where probability is still zero after smoothing\n",
        "                  pass\n",
        "\n",
        "\n",
        "        # Adding log of prior probabilities (correctly by adding logarithms)\n",
        "        if self.class_1_identifier in self.prior_prob_dict and self.prior_prob_dict[self.class_1_identifier] > 0:\n",
        "             class_1_log_likelihood_sum += math.log(self.prior_prob_dict[self.class_1_identifier])\n",
        "\n",
        "        if self.class_2_identifier in self.prior_prob_dict and self.prior_prob_dict[self.class_2_identifier] > 0:\n",
        "            class_2_log_likelihood_sum += math.log(self.prior_prob_dict[self.class_2_identifier])\n",
        "\n",
        "\n",
        "        if class_1_log_likelihood_sum >= class_2_log_likelihood_sum:\n",
        "          computed_labels.append(self.class_1_identifier)\n",
        "        else:\n",
        "          computed_labels.append(self.class_2_identifier)\n",
        "\n",
        "      return computed_labels"
      ],
      "metadata": {
        "id": "aIjyALT2rejR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bcf73c3",
        "outputId": "1ffa20b4-214b-4e8a-b37b-8537236ff493"
      },
      "source": [
        "# Assuming the NaiveBayesClassifier class is defined in a previous cell\n",
        "# Let's create a sample raw dataset string\n",
        "sample_raw_data = \"\"\"+1 1:1 5:1 10:1\n",
        "-1 2:1 5:0 8:1\n",
        "+1 1:0 5:1 12:0\"\"\"\n",
        "\n",
        "# Split the sample data into lines\n",
        "sample_raw_data_set = sample_raw_data.strip().split('\\n')\n",
        "\n",
        "# Instantiate the NaiveBayesClassifier\n",
        "nb_classifier = NaiveBayesClassifier()\n",
        "\n",
        "# Call the create_dataset method with the sample data\n",
        "max_id, labels, dataset = nb_classifier.create_dataset(sample_raw_data_set)\n",
        "\n",
        "# Print the results\n",
        "print(f\"max_id: {max_id}\")\n",
        "print(f\"labels: {labels}\")\n",
        "print(\"Sample dataset entries:\")\n",
        "for i in range(min(3, len(dataset))): # Print up to 3 sample entries\n",
        "  print(f\"  {dataset[i]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_id: 12\n",
            "labels: [0, 1, 0]\n",
            "Sample dataset entries:\n",
            "  [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "  [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11f2d41c",
        "outputId": "77d17823-b2b6-4a6a-933d-3fad77382fa7"
      },
      "source": [
        "# This creates a list with 20 zeros\n",
        "attribute_ar = [0] * 20\n",
        "\n",
        "# This prints the list\n",
        "print(attribute_ar)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    }
  ]
}