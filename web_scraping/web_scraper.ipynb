{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "35QPx2FpUHPJ"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import the necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Send an HTTP request to the website\n",
        "url = 'https://example.com'\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    print('Request successful!')\n",
        "else:\n",
        "    print('Failed to retrieve the webpage')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5A0mn2IUX02",
        "outputId": "752082ac-efea-4452-8bb1-262bad71c98f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Request successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Parse the HTML content\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "# Print the title of the webpage to verify\n",
        "print(\"Title: \" + soup.title.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pe6Ni_wOUm5s",
        "outputId": "a923d60c-9fc2-4402-ff7b-153f89d4c204"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Example Domain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Extract the data you need\n",
        "# Find the table containing the data\n",
        "table = soup.find('table', {'id': 'data-table'})  # Replace 'data-table' with the actual id or class of the table\n",
        "\n",
        "# Extract table rows\n",
        "rows = table.find_all('tr')\n",
        "\n",
        "# Loop through the rows and extract data\n",
        "data = []\n",
        "for row in rows:\n",
        "    cols = row.find_all('td')\n",
        "    cols = [col.text.strip() for col in cols]\n",
        "    data.append(cols)\n",
        "\n",
        "# Convert the data into a pandas DataFrame for easier manipulation\n",
        "df = pd.DataFrame(data, columns=['Column1', 'Column2', 'Column3'])  # Replace with actual column names\n",
        "\n",
        "# Display the scraped data\n",
        "print(df)"
      ],
      "metadata": {
        "id": "Zd9pC69-U4TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Save the scraped data\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('scraped_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "UXsMx4FtVS3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example: Scraping information from Wikipedia\n",
        "\n"
      ],
      "metadata": {
        "id": "KHoBHPrjVnOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Send an HTTP request to the website\n",
        "url = 'https://en.wikipedia.org/wiki/Cloud-computing_comparison'\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    print('Request successful!')\n",
        "else:\n",
        "    print('Failed to retrieve the webpage')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YERLpR7MVtN_",
        "outputId": "8b42dbbe-7f72-4315-d502-ac9d23f60b61"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to retrieve the webpage\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Parse the HTML content\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "# Print the title of the webpage to verify\n",
        "print(\"Title: \" + soup.title.text)\n"
      ],
      "metadata": {
        "id": "ZerBEQrpV0Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the table containing the data (selecting the first table by default)\n",
        "table = soup.find('table')\n",
        "\n",
        "# Extract table rows\n",
        "rows = table.find_all('tr')\n",
        "\n",
        "# Extract headers from the first row (using <th> tags)\n",
        "headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "\n",
        "# Loop through the rows and extract data (skip the first row with headers)\n",
        "data = []\n",
        "for row in rows[1:]:  # Start from the second row onwards\n",
        "    cols = row.find_all('td')\n",
        "    cols = [col.text.strip() for col in cols]\n",
        "    data.append(cols)\n",
        "\n",
        "# Convert the data into a pandas DataFrame, using the extracted headers as column names\n",
        "df = pd.DataFrame(data, columns=headers)\n"
      ],
      "metadata": {
        "id": "plRo3OaEWAzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the DataFrame to verify\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "DMs9s2NTWDTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('scraped_data.csv', index=False)"
      ],
      "metadata": {
        "id": "19yIH-qwWFmC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}