{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "35QPx2FpUHPJ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time  # Optional: To add delays between requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Send an HTTP request to the website\n",
        "# Send a GET request to the web page you want to scrape. This request retrieves the HTML content of the page:\n",
        "\n",
        "url = 'https://example.com'\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    print('Request successful!')\n",
        "else:\n",
        "    print('Failed to retrieve the webpage')\n",
        "\n",
        "'''\n",
        "Explanation: the requests.get() function sends a request to the specified URL and\n",
        "stores the response. It’s important to check the status_code of the response to ensure\n",
        "that the request was successful (a status code of 200 indicates success).\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "x5A0mn2IUX02",
        "outputId": "9a647155-50d3-4294-8659-89c0b9be682f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Request successful!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nExplanation: the requests.get() function sends a request to the specified URL and\\nstores the response. It’s important to check the status_code of the response to ensure \\nthat the request was successful (a status code of 200 indicates success).\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Parse the HTML content\n",
        "# Once you retrieve the HTML content, use BeautifulSoup to parse it and create a navigable tree structure:\n",
        "\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "# Print the title of the webpage to verify\n",
        "print(\"Title: \" + soup.title.text)\n",
        "\n",
        "'''\n",
        "Explanation: BeautifulSoup parses the HTML content and allows you to navigate and\n",
        "search through the HTML elements easily. The soup.title.text line prints the title of\n",
        "the web page to confirm that the HTML has been parsed correctly.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "pe6Ni_wOUm5s",
        "outputId": "ed25482f-9cc1-4175-a1d0-0b0f8bca7d59"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Example Domain\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nExplanation: BeautifulSoup parses the HTML content and allows you to navigate and \\nsearch through the HTML elements easily. The soup.title.text line prints the title of \\nthe web page to confirm that the HTML has been parsed correctly.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Extract the data you need\n",
        "# Determine which HTML elements contain the data you want to extract. For this example,\n",
        "# let’s assume you’re scraping a table with product information:\n",
        "\n",
        "# Locate the table that contains the product data\n",
        "table = soup.find('table', {'id': 'product-table'})  # Replace with the actual id or class name\n",
        "\n",
        "# Extract the rows of the table\n",
        "rows = table.find_all('tr')\n",
        "\n",
        "# Initialize an empty list to store the data\n",
        "data = []\n",
        "\n",
        "# Loop through each row and extract the relevant data\n",
        "for row in rows[1:]:  # Skipping the header row\n",
        "    cols = row.find_all('td')\n",
        "    product_name = cols[0].text.strip()\n",
        "    price = cols[1].text.strip()\n",
        "    rating = cols[2].text.strip()\n",
        "    data.append([product_name, price, rating])\n",
        "\n",
        "# Convert the list to a pandas DataFrame\n",
        "df = pd.DataFrame(data, columns=['Product Name', 'Price', 'Rating'])\n",
        "\n",
        "'''\n",
        "Explanation: this code locates the table with the product data and iterates over\n",
        "each row (skipping the header). For each row, it extracts the product name, price,\n",
        "and rating and appends this information to a list. Finally,\n",
        "the list is converted into a pandas DataFrame for easier manipulation and export.\n",
        "'''"
      ],
      "metadata": {
        "id": "Zd9pC69-U4TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Handle common scraping challenges\n",
        "\n",
        "'''\n",
        "Web scraping often involves dealing with various challenges, such as missing data,\n",
        "dynamic content, or blocked requests. Here are a few strategies to handle these:\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "4OABiHmkYdtK",
        "outputId": "f0e41844-f32f-455a-b034-c9b351dfdfbb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWeb scraping often involves dealing with various challenges, such as missing data, \\ndynamic content, or blocked requests. Here are a few strategies to handle these:\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Handling missing data\n"
      ],
      "metadata": {
        "id": "JRbFCbLzY0Ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If some rows or columns might be missing data, you can add checks to handle these cases:\n",
        "\n",
        "for row in rows[1:]:\n",
        "    cols = row.find_all('td')\n",
        "    if len(cols) == 3:  # Ensure all three columns are present\n",
        "        product_name = cols[0].text.strip() if cols[0] else 'N/A'\n",
        "        price = cols[1].text.strip() if cols[1] else 'N/A'\n",
        "        rating = cols[2].text.strip() if cols[2] else 'N/A'\n",
        "        data.append([product_name, price, rating])\n",
        "    else:\n",
        "        print('Skipping a row with missing data.')"
      ],
      "metadata": {
        "id": "UYZ_ELd-Yr8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Adding delays between requests\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ac8-tHwnY43U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To avoid overwhelming the server or getting blocked, it’s good practice\n",
        "# to add delays between requests:\n",
        "\n",
        "time.sleep(2)  # Adds a 2-second delay before the next request"
      ],
      "metadata": {
        "id": "U255Kir9Y4Gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. Handling dynamic content\n",
        "\n",
        "\n",
        "\n",
        "Some websites load content dynamically using JavaScript, which can’t be directly scraped with BeautifulSoup. In such cases, you might need to use Selenium, a web driver that can interact with JavaScript-driven content."
      ],
      "metadata": {
        "id": "GFInb-2eZMaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "d. Error handling\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QZkHLI28ZTpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Incorporate error handling to manage issues such as network errors or changes in the website structure:\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raises an HTTPError for bad responses\n",
        "except requests.exceptions.HTTPError as err:\n",
        "    print('HTTP error occurred:', err)\n",
        "except Exception as err:\n",
        "    print('Other error occurred:', err)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETKI_8xtZJU0",
        "outputId": "78345492-d090-4f7d-d5ed-44830668d122"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTTP error occurred: 403 Client Error: Forbidden for url: https://en.wikipedia.org/wiki/Cloud-computing_comparison\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Save the scraped data\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('scraped_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "UXsMx4FtVS3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Save the scraped data\n",
        "# Save the DataFrame to a CSV file\n",
        "\n",
        "df.to_csv('scraped_products.csv', index=False)\n",
        "\n",
        "print('Data successfully saved to scraped_products.csv')\n"
      ],
      "metadata": {
        "id": "3LMv4qGNZn4_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}